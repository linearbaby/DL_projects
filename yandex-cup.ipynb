{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "57e06518",
   "metadata": {
    "cellId": "s56msd5hdwlbxfdce3elyk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting annoy\n",
      "  Downloading annoy-1.17.1.tar.gz (647 kB)\n",
      "     |████████████████████████████████| 647 kB 2.5 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: annoy\n",
      "  Building wheel for annoy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for annoy: filename=annoy-1.17.1-cp38-cp38-linux_x86_64.whl size=396909 sha256=9e0d2ba5ee664c66a0cea754beeb2602cc6a18c09e8dac15fe0d589816726e48\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/f9/93/19/30511c4a9ae6b4937455a134c34a39e13943e2c6f46fcd2ed2\n",
      "Successfully built annoy\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.17.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchlars\n",
      "  Downloading torchlars-0.1.2.tar.gz (6.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torchlars) (1.9.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->torchlars) (3.7.4.3)\n",
      "Building wheels for collected packages: torchlars\n",
      "  Building wheel for torchlars (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchlars: filename=torchlars-0.1.2-cp38-cp38-linux_x86_64.whl size=2146902 sha256=3c39a03272978304ae9ede226faa0daa6da29c3bcbfcd341b6bd1a413cb7a7cb\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/22/21/49/3e97b9658bd615f6530088a5604d5e0c5802d0b421a594071c\n",
      "Successfully built torchlars\n",
      "Installing collected packages: torchlars\n",
      "Successfully installed torchlars-0.1.2\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "%pip install annoy\n",
    "%pip install torchlars\n",
    "%pip install welford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "b5ad8542",
   "metadata": {
    "cellId": "8v18wjokqsv4ipn1p1ap5v"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%state_exclude [model train_loader val_loader test_loader model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "dc170968",
   "metadata": {
    "cellId": "2jg1xh7eyvyu6xrl95jpf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "# encoding=utf-8\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import annoy\n",
    "from torchlars import LARS\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5acc679",
   "metadata": {
    "cellId": "tlpqlwtmjo9tyag4s1oqo",
    "execution_id": "58dd5643-f568-4123-85e7-2ce14bcd75bd"
   },
   "source": [
    "#### Datasets 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "abc47cbb",
   "metadata": {
    "cellId": "0h0w2p9fjqcfbebum8ay95g"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "class SimCLR_TrainMusicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features_dir_path, meta_info, device='cpu', crop_size = 60):\n",
    "        self.features_dir_path = features_dir_path\n",
    "        self.meta_info = meta_info\n",
    "        self.trackid2path = meta_info.copy().set_index('trackid')['archive_features_path'].to_dict()\n",
    "        self.artist_track_ids = meta_info.copy().groupby('artistid').agg(list)\n",
    "        # drop where list has len <= 8\n",
    "        \n",
    "        # исходный код с которым почему то лушче результат!!!\n",
    "        # self.artist_track_ids.drop(self.artist_track_ids[self.artist_track_ids.trackid.agg(len) < 8].index)        \n",
    "        \n",
    "        self.artist_track_ids = self.artist_track_ids.drop(self.artist_track_ids[self.artist_track_ids.trackid.agg(len) < 8].index)        \n",
    "        self.crop_size = crop_size\n",
    "        self.data = None # initialized via reshuffle\n",
    "        self.reshuffle()\n",
    "\n",
    "    def _generate_pairs(self, track_ids):\n",
    "        np.random.shuffle(track_ids)\n",
    "        pairs = [track_ids[i-2:i] for i in range(2, len(track_ids)+1, 2)]\n",
    "        return pairs\n",
    "\n",
    "    def reshuffle(self):\n",
    "        # it must be called after each epoch\n",
    "        artist_track_ids = self.artist_track_ids.copy()\n",
    "        artist_track_pairs = artist_track_ids['trackid'].map(self._generate_pairs)\n",
    "        self.data = artist_track_pairs.explode().dropna()\n",
    "        \n",
    "    def _load_item(self, track_id):\n",
    "        track_features_file_path = self.trackid2path[track_id]\n",
    "        track_features = np.load(os.path.join(self.features_dir_path, track_features_file_path))\n",
    "        padding = (track_features.shape[1] - self.crop_size) // 2\n",
    "        return track_features[:, padding:padding+self.crop_size]\n",
    "\n",
    "    def _load_duplete(self, tracks_duplete):\n",
    "        return torch.cat([torch.tensor(self._load_item(x)).unsqueeze(0) for x in tracks_duplete])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, random_idx):\n",
    "        file_idx = self.data.iloc[random_idx]\n",
    "        return self._load_duplete(file_idx), torch.tensor([self.data.index[random_idx]] * 2)\n",
    "    \n",
    "class Clustering_TrainMusicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features_dir_path, meta_info, device='cpu', crop_size = 60):\n",
    "        self.features_dir_path = features_dir_path\n",
    "        self.meta_info = meta_info\n",
    "        self.trackid2path = meta_info.copy().set_index('trackid')['archive_features_path'].to_dict()\n",
    "        self.artist_track_ids = meta_info.copy().groupby('artistid').agg(list)\n",
    "        self.crop_size = crop_size\n",
    "        self.data = None # initialized via reshuffle\n",
    "        self.reshuffle()\n",
    "\n",
    "    def _generate_pairs(self, track_ids):\n",
    "        np.random.shuffle(track_ids)\n",
    "        pairs = [track_ids[i-2:i] for i in range(2, len(track_ids)+1, 2)]\n",
    "        return pairs\n",
    "\n",
    "    def reshuffle(self):\n",
    "      # it must be called after each epoch\n",
    "        artist_track_ids = self.artist_track_ids.copy()\n",
    "        artist_track_pairs = artist_track_ids['trackid'].map(self._generate_pairs)\n",
    "        self.data = artist_track_pairs.explode().dropna()\n",
    "        \n",
    "    def _load_item(self, track_id):\n",
    "        track_features_file_path = self.trackid2path[track_id]\n",
    "        track_features = np.load(os.path.join(self.features_dir_path, track_features_file_path))\n",
    "        padding = (track_features.shape[1] - self.crop_size) // 2\n",
    "        return track_features[:, padding:padding+self.crop_size]\n",
    "\n",
    "    def _load_duplete(self, tracks_duplete):\n",
    "        return torch.cat([torch.tensor(self._load_item(x)).unsqueeze(0) for x in tracks_duplete])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, random_idx):\n",
    "        file_idx = self.data.iloc[random_idx]\n",
    "        return self._load_duplete(file_idx), torch.tensor([self.data.index[random_idx]] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "id": "385b0ef8",
   "metadata": {
    "cellId": "n8kdixr1a2rxt0vhvatbt"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class TrainMusicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features_dir_path, meta_info, device='cpu', crop_size = 60):\n",
    "        self.features_dir_path = features_dir_path\n",
    "        self.data = meta_info\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def _load_item(self, random_idx):\n",
    "        track_features_file_path = self.data.archive_features_path.iloc[random_idx]\n",
    "        track_features = np.load(os.path.join(self.features_dir_path, track_features_file_path))\n",
    "        padding = (track_features.shape[1] - self.crop_size) // 2\n",
    "        return track_features[:, padding:padding+self.crop_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, random_idx):\n",
    "        return torch.tensor(self._load_item(random_idx)), torch.tensor([self.data.artistid.iloc[random_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "id": "7e41d3b2",
   "metadata": {
    "cellId": "88vnrjtw2th95y9uqebxk8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class TestMusicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features_dir_path, meta_info, device='cpu', crop_size = 60):\n",
    "        self.features_dir_path = features_dir_path\n",
    "        self.data = meta_info\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def _load_item(self, random_idx):\n",
    "        track_features_file_path = self.data.archive_features_path.iloc[random_idx]\n",
    "        track_features = np.load(os.path.join(self.features_dir_path, track_features_file_path))\n",
    "        padding = (track_features.shape[1] - self.crop_size) // 2\n",
    "        return track_features[:, padding:padding+self.crop_size]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, random_idx):\n",
    "        return torch.tensor(self._load_item(random_idx)), torch.tensor([self.data.trackid.iloc[random_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "id": "74f5499f",
   "metadata": {
    "cellId": "byivvp3kwh8y3i1xww41vn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def train_val_split(dataset, val_size = 0.2): # Сплит по artistid\n",
    "    artist_ids = dataset['artistid'].unique()\n",
    "    train_artist_ids, val_artist_ids = train_test_split(artist_ids, test_size = val_size)\n",
    "    trainset = dataset[dataset['artistid'].isin(train_artist_ids)].copy()\n",
    "    valset = dataset[dataset['artistid'].isin(val_artist_ids)].copy()\n",
    "    return trainset, valset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b1d7f2",
   "metadata": {
    "cellId": "gsa7t2km6zl3ipkzjvwetd",
    "execution_id": "64744835-e84e-438d-8650-7728b9d3d873"
   },
   "source": [
    "#### Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164acfa",
   "metadata": {
    "cellId": "4q76inuan05551y36cq0s6",
    "execution_id": "e2d2c65d-27a0-4e20-8ccc-8535884c7a0e"
   },
   "source": [
    "Лосс используемый для SIM_CLR фреймворка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "id": "34ccfe4b",
   "metadata": {
    "cellId": "ysddrqu16opbif5tfda4xt"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class NT_Xent(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(NT_Xent, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.shape[0]\n",
    "        N = 2 * batch_size\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    " \n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "        sim_i_j = torch.diag(sim, batch_size)\n",
    "        sim_j_i = torch.diag(sim, -batch_size)\n",
    "\n",
    "        mask = self.mask_correlated_samples(batch_size)\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[mask].reshape(N, -1)\n",
    "\n",
    "        labels = torch.zeros(N).to(positive_samples.device).long()\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            top1_negative_samples, _ = negative_samples.topk(1)\n",
    "            avg_rank = logits.argsort(descending=True).argmin(dim=1).float().mean().cpu().numpy()\n",
    "\n",
    "        return loss, avg_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0547bb48",
   "metadata": {
    "cellId": "2h98grt3j8fc2k03ntnhxp",
    "execution_id": "97894bc8-2420-4d1f-adb6-f4a4c0592c33"
   },
   "source": [
    "Лосс для representation кластеризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "id": "7800c241",
   "metadata": {
    "cellId": "rhq62eyxbfnvgw9ynmyuf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class StudentSimilarityLoss(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super().__init__()\n",
    "        self.repr_alpha = 1\n",
    "        self.latent_alpha = alpha\n",
    "        self.metric = torch.cdist\n",
    "        self.loss = nn.KLDivLoss(reduction='batchmean')\n",
    "  \n",
    "    def _student_prob(self, x, alpha):\n",
    "        x_matrix = self.metric(x, x)\n",
    "        x_matrix = torch.pow(x_matrix / alpha + 1, - (alpha + 1) / 2)\n",
    "        x_matrix = x_matrix / x_matrix.sum()\n",
    "        return x_matrix\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return self.loss(self._student_prob(x1, self.latent_alpha).log(), self._student_prob(x2, self.repr_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4c98d",
   "metadata": {
    "cellId": "exrf91pxhq725ilmtzsund",
    "execution_id": "16a9e741-76fa-4473-bac0-76ca9773fa4a"
   },
   "source": [
    "#### Usable functions ✅ (produced by yandex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "id": "66f564da",
   "metadata": {
    "cellId": "w0f3holxtr1w1rzy0epya"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "def get_ranked_list(embeds, top_size, annoy_num_trees = 32):\n",
    "    annoy_index = None\n",
    "    annoy2id = []\n",
    "    id2annoy = dict()\n",
    "    for track_id, track_embed in embeds.items():\n",
    "        id2annoy[track_id] = len(annoy2id)\n",
    "        annoy2id.append(track_id)\n",
    "        if annoy_index is None:\n",
    "            annoy_index = annoy.AnnoyIndex(len(track_embed), 'angular')\n",
    "        annoy_index.add_item(id2annoy[track_id], track_embed)\n",
    "    annoy_index.build(annoy_num_trees)\n",
    "    ranked_list = dict()\n",
    "    for track_id in embeds.keys():\n",
    "        candidates = annoy_index.get_nns_by_item(id2annoy[track_id], top_size+1)[1:] # exclude trackid itself\n",
    "        candidates = list(filter(lambda x: x != id2annoy[track_id], candidates))\n",
    "        ranked_list[track_id] = [annoy2id[candidate] for candidate in candidates]\n",
    "    return ranked_list\n",
    "\n",
    "def position_discounter(position):\n",
    "    return 1.0 / np.log2(position+1)   \n",
    "\n",
    "def get_ideal_dcg(relevant_items_count, top_size):\n",
    "    dcg = 0.0\n",
    "    for result_indx in range(min(top_size, relevant_items_count)):\n",
    "        position = result_indx + 1\n",
    "        dcg += position_discounter(position)\n",
    "    return dcg\n",
    "\n",
    "def compute_dcg(query_trackid, ranked_list, track2artist_map, top_size):\n",
    "    query_artistid = track2artist_map[query_trackid]\n",
    "    dcg = 0.0\n",
    "    for result_indx, result_trackid in enumerate(ranked_list[:top_size]):\n",
    "        assert result_trackid != query_trackid\n",
    "        position = result_indx + 1\n",
    "        discounted_position = position_discounter(position)\n",
    "        result_artistid = track2artist_map[result_trackid]\n",
    "        if result_artistid == query_artistid:\n",
    "            dcg += discounted_position\n",
    "    return dcg\n",
    "\n",
    "def eval_submission(submission, gt_meta_info, top_size = 100):\n",
    "    track2artist_map = gt_meta_info.set_index('trackid')['artistid'].to_dict()\n",
    "    artist2tracks_map = gt_meta_info.groupby('artistid').agg(list)['trackid'].to_dict()\n",
    "    ndcg_list = []\n",
    "    for query_trackid in tqdm(submission.keys()):\n",
    "        ranked_list = submission[query_trackid]\n",
    "        query_artistid = track2artist_map[query_trackid]\n",
    "        query_artist_tracks_count = len(artist2tracks_map[query_artistid])\n",
    "        ideal_dcg = get_ideal_dcg(query_artist_tracks_count-1, top_size=top_size)\n",
    "        dcg = compute_dcg(query_trackid, ranked_list, track2artist_map, top_size=top_size)\n",
    "        try:\n",
    "            ndcg_list.append(dcg/ideal_dcg)\n",
    "        except ZeroDivisionError:\n",
    "            continue\n",
    "    return np.mean(ndcg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ad258",
   "metadata": {
    "cellId": "y7z1751ly2h2x2lpv5crui",
    "execution_id": "7d539e2f-2a63-42a6-8ce1-2f1e13ba7949"
   },
   "source": [
    "#### Main Net ✅ (produced by yandex)\n",
    "this place is needed to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "86939d8e",
   "metadata": {
    "cellId": "igaxczqvd4gwsjotauxeg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class BasicNet(nn.Module):\n",
    "    def __init__(self, output_features_size):\n",
    "        super().__init__()\n",
    "        self.model_type = 'conv'\n",
    "        self.output_features_size = output_features_size\n",
    "        self.conv_1 = nn.Conv1d(512, output_features_size, kernel_size=3, padding=1)\n",
    "        self.conv_2 = nn.Conv1d(output_features_size, output_features_size, kernel_size=3, padding=1)\n",
    "        self.mp_1 = nn.MaxPool1d(2, 2)\n",
    "        self.conv_3 = nn.Conv1d(output_features_size, output_features_size, kernel_size=3, padding=1)\n",
    "        self.conv_4 = nn.Conv1d(output_features_size, output_features_size, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = self.mp_1(x)\n",
    "        x = F.relu(self.conv_3(x))\n",
    "        return self.conv_4(x).mean(axis = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4602cd14",
   "metadata": {
    "cellId": "3uenpbs6ikorbc3k2kx5w",
    "execution_id": "39fe8643-8b99-4867-aba0-c665c2adc6ed"
   },
   "source": [
    "##### My Net 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "id": "566c7fa4",
   "metadata": {
    "cellId": "tmz980dom61nvzzbdm0u9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 60):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(-1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model: int, input_dim: int = 512, nhead: int = 8, d_hid: int = 512,\n",
    "                 nlayers: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # transpose input from [BATCH, 512, 60] -> [BATCH, 60, 512]\n",
    "        # реализовано в форварде\n",
    "\n",
    "        # лучше использовать прожектор в конце, пушо предположим что исходные вектора\n",
    "        # ембедингов получены натренированой моделью\n",
    "        self.input_encoder = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        self.output_features_size = d_model\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        \n",
    "        # to project tensor to [BATCH, 1, OUTPUTSIZE] need to use result.mean(axis = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transpose input from [BATCH, 512, 60] -> [BATCH, 60, 512]\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.input_encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # to project tensor to [BATCH, OUTPUTSIZE(d_model)] need to use result.mean(axis = 1)\n",
    "        x = self.transformer_encoder(x).mean(dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbe67e",
   "metadata": {
    "cellId": "qlnf8gnq6i94cayo32w",
    "execution_id": "f0fb89be-35c6-4b2c-8bae-df267847fc5e"
   },
   "source": [
    "##### Resnet  my  😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "id": "4e409325",
   "metadata": {
    "cellId": "u6fj92td7zkwr2vbi14go"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel, features_size, downsample):\n",
    "        super().__init__()\n",
    "        self.stride = 2 if downsample else 1\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel = kernel\n",
    "        self.feature_size = features_size\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        self.residual = nn.Sequential(nn.Conv1d(in_planes, out_planes, kernel_size=1, padding=0, stride=2), nn.BatchNorm1d(out_planes)) if downsample else nn.Identity()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes // 4, kernel_size=1, padding=0, stride=self.stride),\n",
    "            nn.BatchNorm1d(out_planes // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_planes // 4, out_planes // 4, kernel_size=kernel, padding=kernel // 2, stride=1),\n",
    "            nn.BatchNorm1d(out_planes // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_planes // 4, out_planes, kernel_size=1, padding=0, stride=1),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_connect = self.residual(x)\n",
    "        x = self.net(x)\n",
    "        x += res_connect\n",
    "        return self.activation(x)\n",
    "    \n",
    "class SimpleBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel, features_size, downsample):\n",
    "        super().__init__()\n",
    "        self.stride = 2 if downsample else 1\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel = kernel\n",
    "        self.feature_size = features_size\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        self.residual = nn.Sequential(nn.Conv1d(in_planes, out_planes, kernel_size=1, padding=0, stride=2), nn.BatchNorm1d(out_planes)) if downsample else nn.Identity()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=1, padding=0, stride=self.stride),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_planes, out_planes, kernel_size=kernel, padding=kernel // 2, stride=1),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_connect = self.residual(x)\n",
    "        x = self.net(x)\n",
    "        x = x + res_connect\n",
    "        return self.activation(x)\n",
    "    \n",
    "class CLRMBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel):\n",
    "        super().__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel = kernel\n",
    "        \n",
    "#         self.residual = nn.Sequential(nn.MaxPool1d(3, 2, 1), nn.BatchNorm1d(out_planes))\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_planes, out_planes, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_planes),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3, 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "#         res_connect = self.residual(x)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "#         return x + res_connect\n",
    "    \n",
    "    \n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, features_size, blocks_size_list, blocks_kernel_list, building_block):\n",
    "        super().__init__()\n",
    "        self.features_size = features_size\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.output_features_size = out_planes      \n",
    "        self.model_type = 'ResNet'\n",
    "        self.BuildingBlock = building_block\n",
    "\n",
    "        blocks_stack = []\n",
    "        for i in range(blocks_size_list[0]):\n",
    "            blocks_stack.append(self.BuildingBlock(in_planes, in_planes, blocks_kernel_list[0], features_size, False)) \n",
    "        for size, kernel in zip(blocks_size_list[1:], blocks_kernel_list[1:]):\n",
    "            features_size = int(features_size // 2)\n",
    "            blocks_stack.append(self.BuildingBlock(in_planes, in_planes * 2, kernel, features_size, True))\n",
    "            in_planes = in_planes * 2\n",
    "            for i in range(size - 1):\n",
    "                blocks_stack.append(self.BuildingBlock(in_planes, in_planes, kernel, features_size, False))   \n",
    "                \n",
    "        blocks_stack.append(nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=1))\n",
    "        self.net = nn.Sequential(*blocks_stack)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x.mean(axis = 2)\n",
    "    \n",
    "    \n",
    "class SampleCNNEncoder(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, blocks_num):\n",
    "        super().__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.output_features_size = out_planes\n",
    "        self.blocks_num = blocks_num\n",
    "        self.model_type = 'SampleCNN'\n",
    "        \n",
    "        blocks_stack = []\n",
    "        blocks_stack.append(CLRMBlock(in_planes, out_planes, 3))\n",
    "        for i in range(blocks_num - 1):\n",
    "            blocks_stack.append(CLRMBlock(out_planes, out_planes, 3))\n",
    "            \n",
    "        self.net = nn.Sequential(*blocks_stack)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x.mean(axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "id": "5d4c02a8",
   "metadata": {
    "cellId": "x0yfklwd7ddnth181fd86d"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.model_type = 'GRU'\n",
    "        self.in_dim = in_dim\n",
    "        self.dropout = dropout\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_features_size = hidden_dim \n",
    "        \n",
    "#         self.downsampler = nn.Sequential(\n",
    "#             nn.Conv1d(in_dim, in_dim, kernel_size=3, padding=1, stride=2),\n",
    "#             nn.Dropout(p=self.dropout),\n",
    "#             nn.Conv1d(in_dim, in_dim, kernel_size=3, padding=1, stride=2),\n",
    "#             nn.Dropout(p=self.dropout)\n",
    "#         )\n",
    "        self.downsampler = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, in_dim, kernel_size=3, padding=1),\n",
    "            nn.MaxPool1d(2, 2),\n",
    "            nn.Conv1d(in_dim, in_dim, kernel_size=3, padding=1),\n",
    "            nn.MaxPool1d(2, 2),\n",
    "            nn.Conv1d(in_dim, in_dim, kernel_size=3, padding=1),\n",
    "            nn.MaxPool1d(2, 2),\n",
    "            nn.Dropout(p=self.dropout)\n",
    "        )\n",
    "        \n",
    "#         self.bgru1 = nn.GRU(in_dim, hidden_dim // 2, batch_first=True, bidirectional=True)\n",
    "#         self.bgru2 = nn.GRU(hidden_dim, hidden_dim // 2, batch_first=True, bidirectional=True)\n",
    "#         self.out_gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "#         self.bgru1 = nn.GRU(in_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "#         self.out_gru = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True)   \n",
    "        \n",
    "        self.bgru1 = nn.GRU(in_dim, hidden_dim, batch_first=True, num_layers=3)\n",
    "        self.out_gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.downsampler(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        out, _ = self.bgru1(x)\n",
    "#         out, _ = self.bgru2(out)\n",
    "        _, out = self.out_gru(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77c00e6",
   "metadata": {
    "cellId": "bicdmx05oqjm4f8vce7cgf",
    "execution_id": "22dd3d5b-8ddc-4130-9101-4c1db4c1ac7f"
   },
   "source": [
    "#### warmup optimizer my  😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "id": "cfb8e808",
   "metadata": {
    "cellId": "ztoxtiha1xaguqw2rxyu4"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, lr_mul, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.lr_mul = lr_mul\n",
    "        self.d_model = d_model\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_steps = 0\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients with the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        d_model = self.d_model\n",
    "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
    "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
    "\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_steps += 1\n",
    "        lr = self.lr_mul * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06061d4",
   "metadata": {
    "cellId": "wwotyyf1kcnnoloukl5u3j",
    "execution_id": "2af59099-b37c-45db-8117-f954b782a74e"
   },
   "source": [
    "#### SymCLR definition ✅ (produced by yandex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "id": "92f38009",
   "metadata": {
    "cellId": "1j19lm56hbis21v6f6ldr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, encoder, projection_dim):\n",
    "        super().__init__()\n",
    "        self.framework_type = \"sim_clr\"\n",
    "        self.encoder = encoder\n",
    "        self.n_features = encoder.output_features_size\n",
    "        self.projection_dim = projection_dim\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.n_features, self.n_features, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_features, self.projection_dim, bias=False),\n",
    "        )\n",
    "        \n",
    "        self.repr_net = nn.Sequential(\n",
    "            nn.Linear(self.n_features, 2000, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, self.n_features, bias=False),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_i, x_j):\n",
    "        h_i = self.encoder(x_i)\n",
    "        h_j = self.encoder(x_j)\n",
    "\n",
    "        z_i = self.projector(h_i)\n",
    "        z_j = self.projector(h_j)\n",
    "        return h_i, h_j, z_i, z_j\n",
    "    \n",
    "    def get_repr(self, h):\n",
    "        return self.repr_net(h)\n",
    "\n",
    "def inference(model, loader, standartizer):\n",
    "    embeds = dict()\n",
    "    for tracks_features, tracks_ids in loader:\n",
    "        tracks_ids = tracks_ids.cpu().numpy().reshape(-1).tolist()\n",
    "        tracks_features = tracks_features.to('cuda')\n",
    "        if standartizer is not None:\n",
    "            tracks_features = standartizer.transform(tracks_features)\n",
    "        with torch.no_grad():\n",
    "            tracks_embeds = model(tracks_features)\n",
    "            for track_id, track_embed in zip(tracks_ids, tracks_embeds):\n",
    "                embeds[track_id] = track_embed.cpu().numpy()\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "id": "90e93e4d",
   "metadata": {
    "cellId": "020dvi04ieq10ujrnm3fp6n"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class EncoderClassificator(nn.Module):\n",
    "    def __init__(self, encoder, projection_dim):\n",
    "        super().__init__()\n",
    "        self.framework_type = \"classifier\"\n",
    "        self.encoder = encoder\n",
    "        self.n_features = encoder.output_features_size\n",
    "        self.projection_dim = projection_dim\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.n_features, self.n_features, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_features, self.projection_dim, bias=False),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.projector(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "id": "d1d65cd9",
   "metadata": {
    "cellId": "cadjqo2b9d9t532nsb1zse"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Standartizer():\n",
    "    def __init__(self, device='cuda:0'):\n",
    "        self.mean = torch.load(\"dataset_mean.pt\").float().to(device)\n",
    "        self.std = torch.load(\"dataset_std.pt\").float().to(device)\n",
    "    \n",
    "    def transform(self, x):\n",
    "        x = torch.transpose(x, -2, -1)\n",
    "        x = (x - self.mean) / self.std\n",
    "        return torch.transpose(x, -2, -1)\n",
    "\n",
    "    def transform_back(self, x):\n",
    "        x * (self.std / self.num_batches) + (self.mean / self.num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f010fdc1",
   "metadata": {
    "cellId": "i2r98y9xc2hb9q8uvwe0fl",
    "execution_id": "4f1be6f5-5afc-48f4-a850-8208830e214a"
   },
   "source": [
    "#### DataLoading ✅ (produced by yandex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753b1d8",
   "metadata": {
    "cellId": "unfq3182wksfer8q6y6z9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "base_dir = \"/home/jupyter/mnt/datasets/yandex_cup_dataset/\"\n",
    "results_dir = \"results_transformers/\" + datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TRAINSET_DIRNAME = 'train_features'\n",
    "TESTSET_DIRNAME = 'test_features'\n",
    "TRAINSET_META_FILENAME = 'train_meta.tsv'\n",
    "TESTSET_META_FILENAME = 'test_meta.tsv'\n",
    "SUBMISSION_FILENAME = 'submission.txt'\n",
    "MODEL_FILENAME = 'model.pt'\n",
    "CHECKPOINT_FILENAME = 'best.pt'\n",
    "device = 'cuda'\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "N_CHANNELS = 1024\n",
    "\n",
    "PROJECTION_DIM = 256\n",
    "# PROJECTION_DIM = 18468   # train_meta_info.artistid.max()\n",
    "NUM_EPOCHS = 50\n",
    "LR = 1e-4\n",
    "TEMPERATURE = 0.1\n",
    "WARMUP_STEPS = 2000\n",
    "\n",
    "TRAINSET_PATH = os.path.join(base_dir, TRAINSET_DIRNAME)\n",
    "TESTSET_PATH = os.path.join(base_dir, TESTSET_DIRNAME)\n",
    "TRAINSET_META_PATH = os.path.join(base_dir, TRAINSET_META_FILENAME)\n",
    "TESTSET_META_PATH = os.path.join('dataset/', TESTSET_META_FILENAME)   # не загрузил 1 файл в датасет, надо дозагрузить\n",
    "SUBMISSION_PATH = os.path.join(results_dir, SUBMISSION_FILENAME)\n",
    "MODEL_PATH = os.path.join(results_dir, MODEL_FILENAME)\n",
    "CHECKPOINT_PATH = os.path.join(results_dir, CHECKPOINT_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef6b87c",
   "metadata": {
    "cellId": "ymcoyr2b5b4miu31q58xg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "model = SimCLR(\n",
    "#     encoder =  TransformerEncoder(d_model= N_CHANNELS, nlayers = 6, nhead=8, dropout=0.2),\n",
    "    encoder = SampleCNNEncoder(512, N_CHANNELS, 4),\n",
    "#     encoder = BasicNet(N_CHANNELS),\n",
    "#     encoder = ResNetEncoder(512, N_CHANNELS, 60, [2, 2, 3], [5, 3, 3], BottleNeck),\n",
    "#     encoder = LSTMEncoder(512, N_CHANNELS, dropout=0.2),\n",
    "    projection_dim = PROJECTION_DIM\n",
    ").to(device)\n",
    "with open(os.path.join(results_dir, \"model_architecture.txt\"), \"w\") as model_file:\n",
    "    model_file.write(repr(model))\n",
    "\n",
    "train_meta_info = pd.read_csv(TRAINSET_META_PATH, sep='\\t')\n",
    "_, validation_train_meta_info = train_val_split(train_meta_info, val_size=0.1)\n",
    "train_meta_info = pd.read_csv(TRAINSET_META_PATH, sep='\\t')\n",
    "test_meta_info = pd.read_csv(TESTSET_META_PATH, sep='\\t')\n",
    "train_meta_info, validation_meta_info = train_val_split(train_meta_info, val_size=0.1)\n",
    "\n",
    "train_dataset = SimCLR_TrainMusicDataset(TRAINSET_PATH, train_meta_info, device)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(TestMusicDataset(TRAINSET_PATH, validation_meta_info, device), batch_size=BATCH_SIZE, num_workers=8)\n",
    "val_train_loader = torch.utils.data.DataLoader(TestMusicDataset(TRAINSET_PATH, validation_train_meta_info, device), batch_size=BATCH_SIZE, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(TestMusicDataset(TESTSET_PATH, test_meta_info, device), batch_size=BATCH_SIZE, num_workers=8)\n",
    "\n",
    "print(\"Loaded data\")\n",
    "print(\"Train set size: {}\".format(len(train_meta_info)))\n",
    "print(\"Validation set size: {}\".format(len(validation_meta_info)))\n",
    "print(\"Test set size: {}\".format(len(test_meta_info)))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b869f2ee",
   "metadata": {
    "cellId": "ci12p7a4ve99ov984fmdof"
   },
   "outputs": [],
   "source": [
    "#!g1.1   \n",
    "model.load_state_dict(torch.load(\"results_transformers/gru_best/best.pt\"))\n",
    "# model.encoder.load_state_dict(torch.load(\"results_transformers/verybig/best.pt\"))\n",
    "# model.encoder.load_state_dict(torch.load(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "d5e5c5fd",
   "metadata": {
    "cellId": "w0bo3sef3wb1mjfr4cddqci"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim_clr    ResNet\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "optimizer = None\n",
    "scheduler = None\n",
    "print(model.framework_type, \"  \", model.encoder.model_type)\n",
    "if model.framework_type == \"sim_clr\":\n",
    "    if model.encoder.model_type == 'Transformer':\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "        optimizer = ScheduledOptim(\n",
    "            torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09), \n",
    "            1, \n",
    "            N_CHANNELS, \n",
    "            WARMUP_STEPS\n",
    "        )\n",
    "    elif model.encoder.model_type in ['ResNet', 'SampleCNN'] :\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "        optimizer = LARS(optimizer=optimizer, eps=1e-8, trust_coef=0.001)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=0.0001)\n",
    "elif model.framework_type == \"classifier\":\n",
    "    if model.encoder.model_type == 'ResNet':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "            factor=0.1, patience=10, threshold=0.001, threshold_mode='abs')\n",
    "    elif model.encoder.model_type == 'Transformer':\n",
    "        optimizer = ScheduledOptim(\n",
    "            torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09), \n",
    "            1, \n",
    "            N_CHANNELS, \n",
    "            WARMUP_STEPS\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "af09ff71",
   "metadata": {
    "cellId": "yfadjtxh30lrjm89ublxx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 2, 512, 60])\n",
      "torch.Size([512, 512, 60])\n",
      "mean:  tensor([-2.3566e-02, -3.8010e-02, -2.1055e-02, -4.3218e-02,  1.5220e-03,\n",
      "        -8.9258e-04, -3.0401e-02, -4.0252e-02,  7.6105e-03,  1.4333e-02,\n",
      "        -2.7575e-02, -2.3669e-02, -1.5745e-02, -1.9528e-02, -3.9606e-02,\n",
      "         3.2809e-02,  3.6028e-02, -2.1577e-02, -3.6904e-02, -4.7996e-02,\n",
      "         8.2398e-03,  1.8565e-02, -1.9955e-02, -4.0573e-02, -4.0094e-02,\n",
      "         1.5161e-02,  1.4935e-02, -5.6856e-03, -5.3570e-04,  1.0829e-02,\n",
      "         2.2429e-03,  9.1977e-02,  1.1174e-02, -1.1680e-02, -1.3353e-02,\n",
      "        -4.7472e-02, -7.0498e-03, -2.9419e-02, -6.2780e-03,  1.0440e-02,\n",
      "        -6.0663e-03, -1.3389e-02, -2.3208e-02,  1.7112e-02,  1.6010e-02,\n",
      "        -2.9152e-02, -9.8055e-03,  6.8727e-03, -1.0013e-02, -4.2652e-03,\n",
      "        -1.5899e-02, -1.0839e-02, -9.5819e-03, -2.9699e-02, -1.6217e-02,\n",
      "        -9.5440e-03, -6.5245e-02, -1.1891e-02, -2.9378e-02, -5.2754e-03,\n",
      "        -3.0358e-02, -3.3414e-02,  6.2473e-03, -3.1717e-03, -1.0608e-02,\n",
      "        -3.8131e-02,  1.1170e-02, -1.2279e-02, -2.1584e-02,  1.0175e-02,\n",
      "        -4.6139e-02, -1.7771e-02,  3.6230e-02, -1.8725e-02,  1.9717e-02,\n",
      "        -3.4814e-02, -8.5110e-04, -1.0965e-02, -1.5545e-02, -1.3994e-02,\n",
      "        -4.0459e-02, -1.0329e-02, -4.2648e-02,  5.5434e-03, -1.1564e-02,\n",
      "        -4.9600e-02, -5.1054e-02, -3.1246e-02, -4.3883e-02,  5.0969e-03,\n",
      "         7.9717e-03,  8.6495e-03, -4.9327e-03, -5.5730e-03,  3.6280e-03,\n",
      "        -9.3101e-03, -2.5076e-02, -2.0829e-02,  1.4351e-02,  2.3154e-02,\n",
      "        -1.4906e-02,  8.5648e-03, -1.7269e-03,  7.4309e-04, -2.7962e-02,\n",
      "         2.5912e-02,  3.0845e-02,  2.9332e-02,  2.0862e-02, -1.3828e-02,\n",
      "        -1.6263e-02,  5.8225e-02, -1.9666e-02, -4.0018e-02, -3.3205e-02,\n",
      "        -7.2951e-04, -5.3495e-04, -7.7784e-04,  1.0904e-02, -2.6286e-02,\n",
      "        -1.1245e-02,  1.1412e-02, -4.5985e-03, -5.1714e-03, -2.9561e-02,\n",
      "        -1.0426e-02, -1.8311e-02,  1.2743e-02, -1.2221e-03,  4.9952e-02,\n",
      "        -3.1830e-02,  7.0007e-04, -5.0874e-03, -3.0004e-03, -6.5385e-03,\n",
      "        -1.4071e-02, -7.6585e-03,  2.8146e-02, -1.5153e-02,  5.4445e-03,\n",
      "        -1.6285e-02,  4.4954e-03, -1.3533e-02, -7.7391e-03, -2.9699e-02,\n",
      "         1.3597e-02,  2.4348e-02, -3.7912e-02,  2.4775e-02, -2.0765e-02,\n",
      "         2.5480e-02, -3.9522e-02, -1.2614e-02, -2.5509e-02, -8.3595e-05,\n",
      "        -4.4916e-03, -4.5782e-02,  1.1439e-02, -3.0445e-02, -1.7894e-03,\n",
      "         1.4001e-02, -2.8620e-02,  3.9291e-02, -2.0100e-02,  3.2823e-02,\n",
      "        -1.1162e-02,  7.4144e-03, -1.0497e-02, -2.0371e-02, -2.5565e-02,\n",
      "         1.3746e-02, -2.4939e-02, -1.7832e-02, -3.1536e-02, -6.2754e-04,\n",
      "         1.9814e-03,  2.4120e-02, -6.9502e-03, -1.8772e-02,  7.7533e-03,\n",
      "         1.3556e-02, -4.0841e-02, -8.2158e-03, -4.2515e-03,  2.9040e-03,\n",
      "         2.1780e-02, -1.8725e-03, -8.1958e-03, -3.1115e-03,  2.1405e-02,\n",
      "        -1.9446e-02, -1.1782e-02, -6.8907e-02, -3.2849e-02, -1.8482e-02,\n",
      "         6.8925e-03, -8.9005e-03,  1.8993e-03, -2.2957e-02, -8.6330e-03,\n",
      "        -3.9670e-02, -1.0160e-02,  6.6756e-03,  8.8253e-03, -2.5934e-02,\n",
      "         2.1126e-02,  1.4596e-02, -2.0290e-02, -1.1566e-04, -2.1944e-02,\n",
      "        -1.9252e-02, -1.4349e-02,  1.7535e-02, -8.8964e-03,  4.1138e-03,\n",
      "        -5.2243e-02, -1.7930e-02, -4.8451e-03, -9.2482e-03, -9.6293e-03,\n",
      "         2.0287e-02, -1.1359e-02,  7.0928e-03, -4.4935e-02,  6.8080e-03,\n",
      "        -5.4821e-02, -6.9554e-03,  8.6285e-03, -2.9478e-02, -1.7128e-02,\n",
      "        -1.6026e-02,  2.3869e-02, -3.6680e-03, -1.2962e-02,  2.8735e-03,\n",
      "        -2.1103e-02, -1.2479e-02, -5.6096e-03,  5.1537e-02, -2.6367e-02,\n",
      "        -4.0538e-02,  2.0150e-02, -1.8590e-02, -5.1791e-03, -9.3130e-03,\n",
      "        -4.9385e-03,  8.6139e-04, -1.5900e-02,  1.1860e-02, -3.8695e-02,\n",
      "        -2.5595e-02,  3.6380e-02, -2.8735e-02, -4.5793e-02, -3.3058e-02,\n",
      "        -1.8576e-02, -3.1145e-02, -1.5143e-02,  5.2920e-03, -2.8298e-02,\n",
      "        -6.1858e-03, -5.8991e-03, -8.1326e-03,  9.9069e-03, -8.7227e-04,\n",
      "         1.3862e-02, -4.0303e-02,  2.6553e-02, -1.8622e-02, -3.7412e-02,\n",
      "         1.0442e-02, -1.3906e-02,  2.0148e-02, -4.9429e-02, -4.1718e-02,\n",
      "         8.3090e-03, -1.1005e-02, -1.8626e-02,  2.7844e-02, -4.1772e-02,\n",
      "        -4.8153e-02, -1.1634e-02, -4.5588e-02,  4.9786e-03,  2.4960e-02,\n",
      "         1.4248e-02,  8.3587e-03, -3.2300e-02,  2.4695e-02, -1.2680e-02,\n",
      "        -6.2554e-03, -3.1842e-02,  7.5922e-03, -3.9144e-02, -7.6188e-03,\n",
      "        -2.3534e-02, -1.5760e-02, -3.0577e-03, -1.4809e-02, -1.0547e-02,\n",
      "         3.2131e-02,  1.3612e-02, -3.7271e-03, -3.9356e-02,  1.6177e-02,\n",
      "         7.3558e-03,  2.9402e-03, -1.5948e-02, -5.1911e-02, -8.6999e-03,\n",
      "         5.0276e-02,  3.8099e-02, -2.9544e-02, -7.7798e-03, -9.2842e-03,\n",
      "         3.9066e-02, -1.4191e-02, -4.8427e-02, -5.1894e-03, -4.3767e-02,\n",
      "        -2.5645e-02,  1.5257e-03, -3.3899e-02,  2.4810e-02, -3.2002e-02,\n",
      "         6.2981e-03, -1.5841e-02, -3.2691e-02, -4.5791e-02, -1.2437e-02,\n",
      "        -5.5759e-03, -2.6924e-03,  2.7050e-02, -1.0185e-02, -3.5411e-02,\n",
      "        -1.5608e-02, -2.5546e-02, -3.5932e-02, -1.1492e-02,  2.1007e-02,\n",
      "        -1.2541e-03, -8.9220e-03, -1.0902e-02,  7.8757e-02, -4.4590e-03,\n",
      "        -9.2937e-03, -1.7810e-02, -2.0079e-02, -4.5269e-03, -3.4630e-02,\n",
      "        -3.0237e-03, -3.9624e-02, -3.6766e-02, -3.2624e-02,  1.9471e-03,\n",
      "         3.3145e-02, -1.5974e-02,  1.0579e-02, -3.3212e-03, -5.2766e-02,\n",
      "        -9.0570e-03,  8.2043e-03, -1.0250e-02, -2.2535e-02, -2.2700e-02,\n",
      "        -1.8889e-02,  2.8061e-02, -1.8056e-02,  1.2607e-02, -1.2801e-02,\n",
      "        -1.1122e-02, -9.3000e-03,  1.7803e-02, -3.2480e-03, -1.3104e-02,\n",
      "         3.6478e-02,  8.2461e-03,  4.3915e-02, -3.6866e-02, -1.9951e-02,\n",
      "        -1.5353e-02, -1.4108e-02, -2.2646e-02, -3.8386e-02,  3.3565e-02,\n",
      "         4.1616e-04, -1.4109e-02, -8.7755e-04,  4.4829e-02,  3.3515e-03,\n",
      "        -1.6021e-02,  5.2389e-02, -3.9489e-02, -4.0336e-03, -8.2524e-03,\n",
      "         6.7695e-03, -2.7670e-02, -5.2138e-02,  6.5920e-03,  2.4386e-02,\n",
      "         4.6788e-02, -2.5647e-03, -2.1770e-02, -3.3030e-02, -3.2278e-02,\n",
      "        -5.3855e-04,  3.1370e-03, -9.7423e-03, -1.7069e-02,  3.6693e-02,\n",
      "        -2.1861e-02, -8.2493e-03,  5.4815e-02,  3.1456e-02,  1.7871e-02,\n",
      "        -2.8735e-02,  1.0120e-02, -1.1690e-02,  1.0952e-02, -1.7961e-02,\n",
      "        -1.2754e-02, -2.4542e-02, -2.5575e-02, -4.1509e-03, -1.5430e-03,\n",
      "         1.2583e-02, -5.9426e-03, -1.0431e-02,  2.1442e-03, -4.0696e-02,\n",
      "         1.1216e-02, -2.9103e-03, -4.0308e-02, -3.1948e-02, -1.4778e-02,\n",
      "        -5.0718e-03, -1.9234e-02, -9.0218e-03, -1.6510e-02,  1.0782e-02,\n",
      "        -3.4964e-02, -4.0173e-02, -1.8816e-03, -6.1541e-03, -3.8135e-02,\n",
      "        -8.6884e-03, -3.2591e-02, -1.6456e-02, -2.3326e-02, -6.7489e-03,\n",
      "         3.6260e-02, -1.3206e-02, -1.8826e-02,  6.2495e-04,  1.6887e-02,\n",
      "        -5.4485e-03, -3.2908e-02,  1.1862e-02,  6.1590e-03, -2.1931e-02,\n",
      "        -4.5294e-02, -1.6692e-02,  3.5404e-02, -2.9964e-02, -1.3497e-02,\n",
      "         7.4891e-03,  1.8789e-03, -4.4844e-02, -1.1192e-02,  1.7633e-02,\n",
      "         8.9342e-03, -1.4158e-02,  1.6443e-02, -3.4784e-02,  3.8778e-02,\n",
      "        -1.1140e-02, -2.4120e-02, -9.9818e-03,  7.3139e-04,  2.1163e-02,\n",
      "        -1.5880e-03,  2.5761e-02,  1.3224e-02, -5.8056e-03,  1.9235e-03,\n",
      "         1.5320e-02,  2.4054e-02, -4.7941e-03, -3.6808e-03,  2.6874e-02,\n",
      "        -6.3857e-03, -3.2655e-02,  5.6860e-04,  6.8085e-03,  6.9369e-03,\n",
      "         1.6257e-02,  4.4284e-03, -1.3202e-02, -6.3522e-03, -8.7287e-03,\n",
      "        -2.0659e-02,  1.3223e-03,  3.3472e-02, -1.7273e-02,  1.4498e-02,\n",
      "         1.9383e-02,  4.1264e-02,  1.6552e-02,  1.6689e-02, -2.2547e-02,\n",
      "         1.9698e-02, -3.6584e-03], device='cuda:0')\n",
      "std:  tensor([0.9959, 0.9750, 0.9950, 0.9680, 1.0392, 1.0002, 0.9803, 0.9598, 1.0063,\n",
      "        1.0222, 0.9797, 1.0093, 0.9847, 1.0018, 0.9833, 1.0150, 1.0077, 0.9886,\n",
      "        0.9919, 0.9942, 1.0046, 1.0201, 0.9739, 0.9848, 0.9637, 1.0292, 1.0038,\n",
      "        0.9834, 1.0042, 1.0000, 1.0092, 1.0770, 1.0041, 0.9816, 0.9952, 0.9838,\n",
      "        1.0051, 0.9898, 0.9972, 0.9952, 0.9803, 0.9917, 0.9887, 1.0009, 1.0179,\n",
      "        0.9886, 0.9820, 1.0134, 1.0093, 1.0061, 0.9888, 0.9890, 0.9887, 0.9732,\n",
      "        1.0006, 0.9885, 0.9465, 0.9833, 0.9686, 1.0246, 0.9967, 0.9896, 1.0096,\n",
      "        1.0185, 0.9951, 0.9908, 1.0223, 0.9952, 1.0029, 1.0271, 0.9783, 0.9922,\n",
      "        1.0364, 0.9860, 1.0128, 0.9826, 0.9974, 0.9773, 0.9787, 0.9980, 0.9725,\n",
      "        0.9995, 0.9812, 1.0100, 0.9754, 0.9809, 0.9036, 0.9736, 0.9783, 0.9959,\n",
      "        0.9917, 1.0044, 0.9848, 0.9873, 1.0117, 0.9850, 1.0116, 0.9957, 1.0216,\n",
      "        1.0136, 0.9816, 1.0007, 1.0027, 0.9837, 0.9745, 1.0156, 1.0122, 1.0029,\n",
      "        0.9914, 0.9836, 0.9733, 1.0521, 1.0002, 0.9904, 0.9821, 1.0008, 1.0076,\n",
      "        1.0236, 1.0058, 0.9731, 0.9913, 1.0166, 0.9883, 1.0211, 0.9666, 1.0084,\n",
      "        0.9839, 0.9828, 1.0016, 1.0167, 0.9871, 1.0173, 0.9887, 1.0014, 0.9886,\n",
      "        0.9856, 0.9781, 1.0296, 1.0044, 0.9953, 1.0019, 0.9817, 0.9789, 1.0044,\n",
      "        0.9643, 1.0201, 1.0252, 0.9775, 1.0046, 0.9854, 1.0045, 0.9753, 1.0033,\n",
      "        0.9944, 0.9775, 1.0110, 0.9760, 1.0069, 0.9679, 1.0001, 1.0044, 0.9838,\n",
      "        1.0225, 1.0000, 1.0237, 0.9907, 1.0046, 0.9845, 0.9649, 0.9941, 0.9902,\n",
      "        1.0005, 0.9981, 0.9710, 0.9961, 0.9895, 1.0144, 1.0012, 0.9844, 1.0083,\n",
      "        0.9998, 0.9805, 0.9913, 1.0119, 0.9989, 0.9889, 1.0007, 0.9767, 0.9957,\n",
      "        1.0285, 1.0090, 1.0053, 0.9433, 0.9816, 1.0125, 0.9961, 1.0073, 0.9934,\n",
      "        0.9834, 0.9950, 0.9627, 0.9958, 1.0007, 0.9938, 1.0005, 1.0016, 1.0064,\n",
      "        0.9954, 1.0109, 0.9964, 0.9965, 0.9909, 0.9936, 0.9981, 0.9890, 0.9740,\n",
      "        0.9925, 1.0189, 0.9978, 1.0059, 1.0257, 1.0064, 0.9872, 0.9646, 0.9794,\n",
      "        0.9840, 0.9882, 1.0114, 0.9743, 0.9819, 0.9778, 1.0448, 1.0085, 1.0041,\n",
      "        0.9877, 1.0001, 0.9858, 1.0083, 1.0325, 0.9934, 0.9766, 1.0003, 0.9906,\n",
      "        1.0000, 0.9766, 1.0051, 0.9938, 1.0089, 1.0085, 0.9984, 0.9707, 1.0338,\n",
      "        0.9638, 0.9790, 0.9890, 1.0170, 0.9787, 0.9963, 1.0150, 0.9856, 1.0045,\n",
      "        0.9955, 0.9960, 1.0072, 1.0105, 0.9920, 0.9736, 1.0194, 1.0022, 0.9938,\n",
      "        1.0186, 1.0223, 1.0053, 0.9838, 0.9888, 1.0027, 0.9969, 0.9790, 1.0062,\n",
      "        0.9678, 0.9343, 0.9971, 0.9625, 1.0072, 1.0108, 0.9926, 1.0121, 0.9744,\n",
      "        1.0161, 1.0015, 0.9864, 0.9705, 1.0130, 0.9541, 1.0011, 0.9995, 0.9900,\n",
      "        0.9999, 1.0036, 0.9966, 1.0204, 0.9920, 1.0112, 0.9672, 1.0140, 1.0154,\n",
      "        1.0124, 1.0212, 0.9898, 0.9870, 1.0152, 1.0208, 1.0155, 1.0036, 1.0077,\n",
      "        1.0060, 1.0007, 0.9581, 0.9900, 0.9771, 0.9680, 0.9978, 1.0025, 1.0147,\n",
      "        0.9841, 1.0025, 1.0008, 0.9933, 0.9777, 1.0076, 1.0050, 1.0232, 1.0182,\n",
      "        0.9983, 0.9802, 0.9899, 0.9534, 0.9641, 0.9972, 1.0120, 0.9521, 1.0292,\n",
      "        0.9855, 1.0534, 1.0075, 1.0239, 0.9890, 0.9971, 0.9995, 0.9919, 1.0175,\n",
      "        0.9837, 0.9874, 0.9861, 0.9752, 1.0031, 0.9688, 0.9954, 1.0201, 0.9714,\n",
      "        0.9770, 1.0191, 0.9967, 0.9871, 0.9833, 0.9908, 1.0357, 1.0177, 1.0374,\n",
      "        1.0030, 1.0088, 0.9940, 1.0209, 1.0091, 0.9912, 1.0526, 1.0060, 1.0178,\n",
      "        0.9991, 0.9875, 0.9986, 1.0019, 0.9947, 0.9632, 1.0034, 1.0009, 0.9913,\n",
      "        1.0077, 1.0314, 1.0149, 0.9879, 1.0102, 0.9785, 1.0001, 0.9956, 0.9982,\n",
      "        0.9906, 0.9743, 1.0086, 1.0206, 1.0180, 1.0115, 0.9774, 1.0024, 0.9717,\n",
      "        1.0017, 0.9948, 0.9981, 0.9799, 1.0196, 0.9771, 0.9647, 1.0279, 1.0352,\n",
      "        1.0023, 0.9950, 1.0125, 1.0086, 1.0096, 0.9968, 0.9956, 0.9989, 0.9744,\n",
      "        1.0072, 1.0706, 1.0035, 0.9750, 0.9973, 1.0043, 0.9590, 0.9723, 1.0087,\n",
      "        0.9942, 0.9952, 1.0027, 0.9950, 1.0095, 0.9858, 0.9909, 1.0140, 0.9809,\n",
      "        1.0058, 0.9834, 0.9944, 0.9884, 0.9874, 0.9736, 0.9916, 1.0009, 0.9954,\n",
      "        1.0028, 0.9912, 0.9956, 1.0121, 1.0045, 1.0009, 0.9857, 1.0146, 1.0145,\n",
      "        0.9944, 0.9681, 0.9907, 1.0438, 0.9862, 0.9696, 1.0048, 0.9987, 0.9522,\n",
      "        0.9697, 1.0091, 1.0053, 0.9680, 1.0055, 0.9829, 1.0215, 0.9690, 0.9768,\n",
      "        1.0109, 0.9835, 1.0075, 1.0072, 0.9858, 1.0307, 1.0214, 1.0130, 0.9911,\n",
      "        1.0012, 0.9992, 1.0059, 0.9981, 1.0045, 0.9886, 0.9776, 1.0074, 1.0164,\n",
      "        1.0414, 0.9974, 0.9891, 0.9885, 1.0170, 0.9969, 1.0133, 1.0341, 0.9965,\n",
      "        1.0124, 1.0222, 1.0294, 1.0106, 1.0125, 0.9781, 1.0144, 1.0098],\n",
      "       device='cuda:0')\n",
      "torch.Size([512, 1024])\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "# sanity check\n",
    "standartizer = Standartizer()\n",
    "from itertools import islice\n",
    "with torch.no_grad():\n",
    "    for item, label in islice(train_loader, 1):\n",
    "        print(item.shape)\n",
    "        item = item.to(device)\n",
    "        print(standartizer.transform(item[:,0,:,:]).shape)\n",
    "        print(\"mean: \", torch.transpose(standartizer.transform(item[:,0,:,:]), -1, -2).reshape(-1, 512).mean(dim=0))\n",
    "        print(\"std: \", torch.transpose(standartizer.transform(item[:,0,:,:]), -1, -2).reshape(-1, 512).std(dim=0))\n",
    "        item = item.to(device)\n",
    "        print(model.encoder(item[:,0,:,:]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34f8a0",
   "metadata": {
    "cellId": "chdq765643sukc3tzzs3tf",
    "execution_id": "3f21ae81-ea38-4dc0-8254-6b83099644c2"
   },
   "source": [
    "#### Train and save subbmission ✅ (produced by yandex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "b6924c60",
   "metadata": {
    "cellId": "4w2bopbl8pyputggo4j6i8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def train(module, standartizer, train_loader, train_dataset, val_loader, valset_meta, optimizer, scheduler, criterion, criterion_repr, num_epochs, checkpoint_path, device, top_size = 100):\n",
    "    # need to get not deviced tensors because of multiprocessing\n",
    "    max_ndcg = None\n",
    "    counter_max_ndcg = 0\n",
    "    saturate = False  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if counter_max_ndcg > 2:  # initiate clustering loss\n",
    "            saturate = True\n",
    "            \n",
    "        pbar = tqdm(enumerate(train_loader))\n",
    "        avg_rang_sum = 0\n",
    "        loss_sum = 0\n",
    "        train_dataset.reshuffle()\n",
    "        for batch_num, batch in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            module.train()\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            \n",
    "            # if sim_clr\n",
    "            \n",
    "            x_i, x_j = inputs[:, 0, :, :], inputs[:, 1, :, :]\n",
    "            if standartizer is not None:\n",
    "                x_i, x_j = standartizer.transform(x_i), standartizer.transform(x_j)\n",
    "                \n",
    "            h_i, h_j, z_i, z_j = module(x_i, x_j)\n",
    "            loss, avg_rank = criterion(z_i, z_j)\n",
    "            if saturate:\n",
    "                h = torch.cat((h_i, h_j))\n",
    "                loss = loss + 0.2 * criterion_repr(h, module.get_repr(h))\n",
    "            avg_rang_sum += avg_rank            \n",
    "                \n",
    "            loss_sum += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             scheduler.step(loss)\n",
    "            \n",
    "            pbar.set_postfix({\"Epoch\": epoch+1, \"mean loss\": loss_sum / (batch_num + 1), \"mean avg_rank\" :avg_rang_sum / (batch_num + 1)})\n",
    "#             pbar.set_postfix({\"Epoch\": epoch+1, \"mean loss\": loss_sum / (batch_num + 1)})\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_encoder = module.encoder\n",
    "            embeds_encoder = inference(model_encoder, val_loader, standartizer)\n",
    "            ranked_list_encoder = get_ranked_list(embeds_encoder, top_size)\n",
    "            val_ndcg_encoder = eval_submission(ranked_list_encoder, valset_meta)\n",
    "            \n",
    "            model_projector = nn.Sequential(module.encoder, module.projector)\n",
    "            embeds_projector = inference(model_projector, val_loader, standartizer)\n",
    "            ranked_list_projector = get_ranked_list(embeds_projector, top_size)\n",
    "            val_ndcg_projector = eval_submission(ranked_list_projector, valset_meta)\n",
    "            \n",
    "            print(\"Validation nDCG on epoch {}\".format(epoch))\n",
    "            print(\"Encoder - {}\".format(val_ndcg_encoder))\n",
    "            print(\"Projector - {}\".format(val_ndcg_projector))\n",
    "            if (max_ndcg is None) or (val_ndcg_encoder > max_ndcg):\n",
    "                max_ndcg = val_ndcg_encoder\n",
    "                counter_max_ndcg = 0\n",
    "                torch.save(model_encoder.state_dict(), checkpoint_path)\n",
    "            else:\n",
    "                counter_max_ndcg += 1\n",
    "\n",
    "def save_submission(submission, submission_path):\n",
    "    with open(submission_path, 'w') as f:\n",
    "        for query_trackid, result in submission.items():\n",
    "            f.write(\"{}\\t{}\\n\".format(query_trackid, \" \".join(map(str, result))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534cdebc",
   "metadata": {
    "cellId": "w95b5ranilhhjppuqujmua",
    "execution_id": "c676f186-154b-4bea-a2cc-a730e52800e0"
   },
   "source": [
    "#### Main training-eval cycle ✅ (produced by yandex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8347de5c",
   "metadata": {
    "cellId": "1cjb9zis726i48maiv79ao",
    "execution_id": "8bcfe8df-d5c6-4cea-8ea1-af5adb547701"
   },
   "source": [
    "В клеточке ниже я считаю при помощи алгоритма вилфорда среднее и стандартное отклонение по всему датасету"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "e9ec4d48",
   "metadata": {
    "cellId": "gtw5odumm0byucniiznz6l"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [01:39<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# calculate mean variance\n",
    "\n",
    "# from welford import Welford\n",
    "# normalizer = Welford()\n",
    "\n",
    "# for data, _ in tqdm(train_loader):\n",
    "#     data = data.reshape(-1, 512, 60)\n",
    "#     normalizer.add_all(torch.transpose(data, 1, 2).reshape((-1, 512)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb0dc0",
   "metadata": {
    "cellId": "gscd2cydkyn95c5c6wgb0m",
    "execution_id": "1adbf47e-a5b7-49cc-a1b4-2ca7fad9b9ee"
   },
   "source": [
    "Главный тренировочный цикл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "216bb79c",
   "metadata": {
    "cellId": "b8rnea7ahg71rfmfnhh49p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Validation nDCG on epoch 0\n",
      "Encoder - 0.11931760187880033\n",
      "Projector - 0.13193957451477736\n",
      "Validation nDCG on epoch 1\n",
      "Encoder - 0.15061952017905908\n",
      "Projector - 0.16771937706158294\n",
      "Validation nDCG on epoch 2\n",
      "Encoder - 0.1705590639685832\n",
      "Projector - 0.19123150562784486\n",
      "Validation nDCG on epoch 3\n",
      "Encoder - 0.18295250966770032\n",
      "Projector - 0.2047412867890922\n",
      "Validation nDCG on epoch 4\n",
      "Encoder - 0.19192885484813096\n",
      "Projector - 0.21228116820168416\n",
      "Validation nDCG on epoch 5\n",
      "Encoder - 0.19748388124764607\n",
      "Projector - 0.2165337856771514\n",
      "Validation nDCG on epoch 6\n",
      "Encoder - 0.20002003092284718\n",
      "Projector - 0.21630448630335758\n",
      "Validation nDCG on epoch 7\n",
      "Encoder - 0.20076328669412055\n",
      "Projector - 0.21316432089122744\n",
      "Validation nDCG on epoch 8\n",
      "Encoder - 0.20152217422708524\n",
      "Projector - 0.20688233401333048\n",
      "Validation nDCG on epoch 9\n",
      "Encoder - 0.19966772875338995\n",
      "Projector - 0.20134903673692034\n",
      "Validation nDCG on epoch 10\n",
      "Encoder - 0.1989789799517578\n",
      "Projector - 0.19406709183532758\n",
      "Validation nDCG on epoch 11\n",
      "Encoder - 0.1974221804390383\n",
      "Projector - 0.18508127302058025\n",
      "Validation nDCG on epoch 12\n",
      "Encoder - 0.1969118460183912\n",
      "Projector - 0.17818367371530247\n",
      "Validation nDCG on epoch 13\n",
      "Encoder - 0.19481767578727177\n",
      "Projector - 0.17085064089521684\n",
      "Validation nDCG on epoch 14\n",
      "Encoder - 0.19280479357937474\n",
      "Projector - 0.16451941511574444\n",
      "Validation nDCG on epoch 15\n",
      "Encoder - 0.191110511357556\n",
      "Projector - 0.15860421573850303\n",
      "Validation nDCG on epoch 16\n",
      "Encoder - 0.1885741475062614\n",
      "Projector - 0.15199379295743154\n",
      "Validation nDCG on epoch 17\n",
      "Encoder - 0.1868553357775902\n",
      "Projector - 0.14698512539547315\n",
      "Validation nDCG on epoch 18\n",
      "Encoder - 0.18452501641913197\n",
      "Projector - 0.14172290762846887\n",
      "Validation nDCG on epoch 19\n",
      "Encoder - 0.18250533462060212\n",
      "Projector - 0.1367070293898313\n",
      "Validation nDCG on epoch 20\n",
      "Encoder - 0.18065962451341747\n",
      "Projector - 0.1324126719553818\n",
      "Validation nDCG on epoch 21\n",
      "Encoder - 0.1785576209897549\n",
      "Projector - 0.12700239860676765\n",
      "Validation nDCG on epoch 22\n",
      "Encoder - 0.17577520267076865\n",
      "Projector - 0.12299210335818433\n",
      "Validation nDCG on epoch 23\n",
      "Encoder - 0.17388623920892243\n",
      "Projector - 0.11866164975986351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "137it [00:54,  2.51it/s, Epoch=3, mean loss=4.81, mean avg_rank=76.6]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5853.53it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5857.13it/s]\n",
      "137it [00:54,  2.52it/s, Epoch=4, mean loss=4.6, mean avg_rank=66.5]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5880.71it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5762.27it/s]\n",
      "137it [00:54,  2.52it/s, Epoch=5, mean loss=4.41, mean avg_rank=57.8]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5893.93it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5890.44it/s]\n",
      "137it [00:54,  2.51it/s, Epoch=6, mean loss=4.27, mean avg_rank=51.4]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5899.56it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5826.72it/s]\n",
      "137it [00:54,  2.52it/s, Epoch=7, mean loss=4.11, mean avg_rank=44.8]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5889.09it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5813.28it/s]\n",
      "137it [00:54,  2.51it/s, Epoch=8, mean loss=3.96, mean avg_rank=38.8]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5931.26it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5812.78it/s]\n",
      "137it [00:54,  2.52it/s, Epoch=9, mean loss=3.82, mean avg_rank=33.2]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5888.47it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5840.48it/s]\n",
      "137it [00:54,  2.52it/s, Epoch=10, mean loss=3.65, mean avg_rank=27.4]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5857.78it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5858.89it/s]\n",
      "137it [00:54,  2.52it/s, Epoch=11, mean loss=3.5, mean avg_rank=22.5]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5851.49it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5861.01it/s]\n",
      "137it [00:54,  2.51it/s, Epoch=12, mean loss=3.34, mean avg_rank=18.3]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5799.22it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5855.88it/s]\n",
      "137it [00:56,  2.44it/s, Epoch=13, mean loss=3.19, mean avg_rank=14.9]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5853.84it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5826.65it/s]\n",
      "137it [00:56,  2.44it/s, Epoch=14, mean loss=3.03, mean avg_rank=11.7]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5851.98it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5869.10it/s]\n",
      "137it [00:56,  2.44it/s, Epoch=15, mean loss=2.89, mean avg_rank=9.42]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5852.15it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5771.71it/s]\n",
      "137it [00:55,  2.45it/s, Epoch=16, mean loss=2.74, mean avg_rank=7.29]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5965.03it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5910.28it/s]\n",
      "137it [00:55,  2.45it/s, Epoch=17, mean loss=2.6, mean avg_rank=5.86]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5929.19it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5842.60it/s]\n",
      "137it [00:56,  2.44it/s, Epoch=18, mean loss=2.46, mean avg_rank=4.42]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5912.83it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5795.88it/s]\n",
      "137it [00:55,  2.45it/s, Epoch=19, mean loss=2.33, mean avg_rank=3.53]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5894.35it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5755.09it/s]\n",
      "137it [00:56,  2.43it/s, Epoch=20, mean loss=2.2, mean avg_rank=2.65]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5914.67it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5874.04it/s]\n",
      "137it [00:55,  2.45it/s, Epoch=21, mean loss=2.08, mean avg_rank=2.15]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5904.71it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5842.26it/s]\n",
      "137it [00:55,  2.45it/s, Epoch=22, mean loss=1.97, mean avg_rank=1.75]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5933.94it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5885.13it/s]\n",
      "137it [00:56,  2.44it/s, Epoch=23, mean loss=1.85, mean avg_rank=1.33]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5919.49it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5803.87it/s]\n",
      "137it [00:56,  2.44it/s, Epoch=24, mean loss=1.75, mean avg_rank=1.07]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5856.12it/s]\n",
      "100%|██████████| 16680/16680 [00:02<00:00, 5759.79it/s]\n",
      "137it [00:56,  2.44it/s, Epoch=25, mean loss=1.64, mean avg_rank=0.865]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-f580e8beea00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstandartizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandartizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-5cff19606c64>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(module, standartizer, train_loader, train_dataset, val_loader, valset_meta, optimizer, scheduler, criterion, criterion_repr, num_epochs, checkpoint_path, device, top_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mmodel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0membeds_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandartizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mranked_list_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ranked_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mval_ndcg_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranked_list_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalset_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-25767c1b5575>\u001b[0m in \u001b[0;36mget_ranked_list\u001b[0;34m(embeds, top_size, annoy_num_trees)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mranked_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrack_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannoy_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_nns_by_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2annoy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrack_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# exclude trackid itself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mid2annoy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrack_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mranked_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrack_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mannoy2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "print(\"Train\")\n",
    "train(\n",
    "    module = model,\n",
    "    standartizer = Standartizer(), #None\n",
    "    train_loader = train_loader,\n",
    "    train_dataset = train_dataset,\n",
    "    val_loader = val_loader,\n",
    "    valset_meta = validation_meta_info,\n",
    "    optimizer = optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion = NT_Xent(temperature = TEMPERATURE),\n",
    "#     criterion = nn.CrossEntropyLoss(),\n",
    "    criterion_repr = StudentSimilarityLoss(alpha=2 * N_CHANNELS),\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    checkpoint_path = CHECKPOINT_PATH,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "69f3f5a7",
   "metadata": {
    "cellId": "sj8g8u176tcf1bj8clzq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "torch.save(optimizer.state_dict(), results_dir + '/optimizer.pt')\n",
    "torch.save(model.encoder.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d5c05",
   "metadata": {
    "cellId": "8m0okbrkwensk3hco7iik",
    "execution_id": "e0936bd3-9182-480d-bfaa-0cb7eb77aae3"
   },
   "source": [
    "#### Generating submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "43115c4c",
   "metadata": {
    "cellId": "q7a1pr6r7akh9c25mo6wwl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "# model.encoder = LSTMEncoder(512, 1024)\n",
    "# model.encoder.load_state_dict(torch.load(\"results_transformers/2022-11-03 10:19:54/best.pt\"))\n",
    "# model.encoder.to(device)\n",
    "print(\"Submission\")\n",
    "test_loader = test_loader\n",
    "model_for_inference = model.encoder\n",
    "embeds = inference(model_for_inference, test_loader, Standartizer())\n",
    "submission = get_ranked_list(embeds, 100)\n",
    "save_submission(submission, SUBMISSION_PATH)\n",
    "# save_submission(submission, \"results_transformers/2022-11-03 10:19:54/submission.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ad1234",
   "metadata": {
    "cellId": "my0cqprlqg0h97ebf0rifq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "1da69cae-48f9-45f0-9974-bd5230e11ff6",
  "notebookPath": "yandex-cup.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
